{"cells":[{"cell_type":"code","execution_count":5,"id":"ce63966c-3e4a-4e0f-aed5-22a186dc8eda","metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:29:32.923057Z","iopub.status.busy":"2025-03-26T05:29:32.922488Z","iopub.status.idle":"2025-03-26T05:32:25.234276Z","shell.execute_reply":"2025-03-26T05:32:25.233328Z","shell.execute_reply.started":"2025-03-26T05:29:32.923016Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\nCollecting datasets\n  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\nCollecting torch\n  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting transformers\n  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\nCollecting adapters\n  Downloading adapters-1.1.0-py3-none-any.whl.metadata (16 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting accelerate\n  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.66.4)\nCollecting scikit-learn (from sentence-transformers)\n  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting scipy (from sentence-transformers)\n  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting huggingface-hub>=0.20.0 (from sentence-transformers)\n  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\nCollecting Pillow (from sentence-transformers)\n  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.0.3)\nCollecting requests>=2.32.2 (from datasets)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\nCollecting multiprocess<0.70.17 (from datasets)\n  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nCollecting aiohttp (from datasets)\n  Downloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\nCollecting networkx (from torch)\n  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.0.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.2.0 (from torch)\n  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting sympy==1.13.1 (from torch)\n  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\nCollecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting safetensors>=0.4.3 (from transformers)\n  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting transformers\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n  Downloading multidict-6.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting propcache>=0.2.0 (from aiohttp->datasets)\n  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.0.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\nCollecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m126.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading adapters-1.1.0-py3-none-any.whl (293 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nDownloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multidict-6.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.5/232.5 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nDownloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, threadpoolctl, sympy, scipy, safetensors, requests, pyarrow, propcache, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, multidict, joblib, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, aiosignal, tokenizers, nvidia-cusolver-cu12, aiohttp, transformers, torch, sentence-transformers, datasets, adapters, accelerate, evaluate\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 12.0.1\n    Uninstalling pyarrow-12.0.1:\n      Successfully uninstalled pyarrow-12.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nibis-framework 6.2.0 requires pyarrow<13,>=2, but you have pyarrow 19.0.1 which is incompatible.\njupyterlab-server 2.27.1 requires jinja2>=3.0.3, but you have jinja2 3.0.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pillow-11.1.0 accelerate-1.5.2 adapters-1.1.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 datasets-3.4.1 dill-0.3.8 evaluate-0.4.3 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.29.3 joblib-1.4.2 mpmath-1.3.0 multidict-6.2.0 multiprocess-0.70.16 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 propcache-0.3.1 pyarrow-19.0.1 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence-transformers-3.4.1 sympy-1.13.1 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.6.0 transformers-4.47.1 triton-3.2.0 yarl-1.18.3\n"}],"source":"!pip install -U sentence-transformers datasets torch transformers adapters evaluate accelerate"},{"cell_type":"code","execution_count":6,"id":"74e00f38-820c-4ebb-a105-a73e1deac12b","metadata":{"execution":{"iopub.execute_input":"2025-04-06T14:40:37.594392Z","iopub.status.busy":"2025-04-06T14:40:37.593273Z","iopub.status.idle":"2025-04-06T14:40:38.137755Z","shell.execute_reply":"2025-04-06T14:40:38.135368Z","shell.execute_reply.started":"2025-04-06T14:40:37.594309Z"},"language":"python","trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'sentence_transformers'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoConfig, TrainingArguments\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"]}],"source":"from sentence_transformers import SentenceTransformer, util\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoConfig, TrainingArguments\nimport torch\nimport csv\nimport numpy as np\nimport pandas as pd"},{"cell_type":"code","execution_count":5,"id":"68ac06aa-97f9-40c7-b833-061db3b4eb6c","metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:36:44.950186Z","iopub.status.busy":"2025-03-26T05:36:44.944982Z","iopub.status.idle":"2025-03-26T05:37:26.527382Z","shell.execute_reply":"2025-03-26T05:37:26.526511Z","shell.execute_reply.started":"2025-03-26T05:36:44.950145Z"},"language":"python","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad649b65fdb34f5b8a39cfb28bbd42f3","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/27.5M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49264c915e0f412c84144247e4e071a3","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a5d8a438cb941f080cac1cd859d051c","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/297k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c8b594736cd437aa9bcaa8fb2422813","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95cc0f2cf8da434bb33853c78717eb42","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/59.1k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46f7f5177e9a496892795ff099175216","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b0a7bbcf20d4d4088e885af98c8ac34","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/34.8M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77f867d6c7b94d8288de5018e4ab2f77","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f959244609f4e6bada04abee1197ed6","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71f576d245b541058bb55684c91a8cc5","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d222d42cd9284afda8ea2af6ec0381a0","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6411c959f6d644088861813d0a297304","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf8aa4c9a14443b490afb02e9f57036e","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/57.0M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59e3fa1e1a05405a84317a98d42a6daf","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e52178b994d64572ae5554fddf897838","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/511k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7fb5ec729c8745e5bc442a72373a5c37","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba4f2c123314440183dfd23c1770dfaf","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/121k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0230af8d43d4dbc902a7b20266c6002","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c6354c48e774613af62a669a7d7124e","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/63.8M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f6eeb8db17b4ae8b1cabc2ba76030d3","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a56d102b4b7e41baa550072bf2271904","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f44e53ff91e464ab78f2052e630cc22","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e3c84ce8d0b440d889313a94e6c959e","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/132k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"348e46cf86e343beacb618618fec8b27","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60f7290e7f3948aca69a5121be1284fe","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/37.1M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1cdae80fc8744cbc959d5fdb19b619c1","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa34e4ed8e0a4b668ae85b067d858f7e","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/722k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05f6d26320c840ad8da96e805cafa321","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b37167a9d5004ad691bfd944b8e95dfc","version_major":2,"version_minor":0},"text/plain":"Downloading data:   0%|          | 0.00/124k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f0a510e8c084e229f64294964ccf8a4","version_major":2,"version_minor":0},"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]"},"metadata":{},"output_type":"display_data"}],"source":"eclipse_url = \"https://github.com/logpai/bughub/raw/refs/heads/master/EclipsePlatform/\"\nfirefox_url = \"https://raw.githubusercontent.com/logpai/bughub/refs/heads/master/Firefox/\"\njdt_url = \"https://raw.githubusercontent.com/logpai/bughub/refs/heads/master/JDT/\"\nmozilla_url = \"https://github.com/logpai/bughub/raw/refs/heads/master/MozillaCore/\"\nthunderbird_url = \"https://raw.githubusercontent.com/logpai/bughub/refs/heads/master/Thunderbird/\"\n\ndef create_datasets(url, bugs):\n    dataset = load_dataset(\"csv\", data_files = url + bugs)\n    train = load_dataset(\"csv\", data_files = url + \"train.csv\")\n    test = load_dataset(\"csv\", data_files = url + \"test.csv\")\n    dataset_df = dataset['train'].to_pandas()\n    train_df = train['train'].to_pandas()\n    test_df = test['train'].to_pandas()    \n    train_data = train_df.merge(dataset_df, on=\"Issue_id\", how=\"left\")\n    train_data = pd.DataFrame(train_data).values.tolist()\n    test_data = test_df.merge(dataset_df, on=\"Issue_id\", how=\"left\")\n    test_data = pd.DataFrame(test_data).values.tolist()\n    issue_dict = {issue[\"Issue_id\"]: issue for issue in dataset[\"train\"].to_list()}\n    return train_data, test_data, issue_dict\n\neclipse_train, eclipse_test, eclipse_dict = create_datasets(eclipse_url, \"eclipse_platform.zip\")\nfirefox_train, firefox_test, firefox_dict = create_datasets(firefox_url, \"mozilla_firefox.zip\")\njdt_train, jdt_test, jdt_dict = create_datasets(jdt_url,\"eclipse_jdt.csv\")\nmozilla_train, mozilla_test, mozilla_dict = create_datasets(mozilla_url, \"mozilla_core.zip\")\nthunderbird_train, thunderbird_test, thunderbird_dict = create_datasets(thunderbird_url, \"mozilla_thunderbird.csv\")"},{"cell_type":"code","execution_count":7,"id":"bcb7913b-99bc-4972-8b28-c533d4900b37","metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:37:29.308713Z","iopub.status.busy":"2025-03-26T05:37:29.307662Z","iopub.status.idle":"2025-03-26T05:37:29.860067Z","shell.execute_reply":"2025-03-26T05:37:29.859175Z","shell.execute_reply.started":"2025-03-26T05:37:29.308666Z"},"language":"python","trusted":true},"outputs":[],"source":"import random\nfrom sentence_transformers import InputExample\n\nSEED = 69  # Change this number for different results\n\ndef set_seed(seed):\n    random.seed(seed)  # Python's random module\n    np.random.seed(seed)  # NumPy\n    torch.manual_seed(seed)  # PyTorch CPU\n    torch.cuda.manual_seed(seed)  # PyTorch GPU (if available)\n    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n    torch.backends.cudnn.benchmark = False  # Avoids non-deterministic optimizations\n\n\ndef create_pairs(dataset, issue_dict):\n    pairs = []\n    for entry in dataset:\n        issue_id = entry[0] \n        duplicates = entry[1]  # List of duplicate issue IDs\n        \n        if duplicates:\n            duplicates = entry[1].split(\";\")\n            for dup_id in duplicates:\n                if int(dup_id) in issue_dict:\n                    dup_id = int(dup_id)\n                    bug_1 = (issue_dict[issue_id][\"Component\"] or \"\") + \" \" + (issue_dict[issue_id][\"Title\"] or \"\") + \" \" + (issue_dict[issue_id][\"Description\"] or \"\")\n                    bug_2 = (issue_dict[dup_id][\"Component\"] or \"\") + \" \" + (issue_dict[dup_id][\"Title\"] or \"\") + \" \" + (issue_dict[dup_id][\"Description\"] or \"\")\n                    pairs.append(InputExample(\n                        texts = [bug_1, bug_2],\n                        label = 1\n                    ))\n\n            # Create negative samples (random non-duplicate issues)\n            for _ in range(len(duplicates)):  # Create equal number of negative pairs\n                random_id = random.choice(list(issue_dict.keys()))\n                if str(random_id) not in duplicates and random_id != issue_id:\n                    bug_1 = (issue_dict[issue_id][\"Component\"] or \"\") + \" \" + (issue_dict[issue_id][\"Title\"] or \"\") + \" \" + (issue_dict[issue_id][\"Description\"] or \"\")\n                    bug_2 = (issue_dict[random_id][\"Component\"] or \"\") + \" \" + (issue_dict[random_id][\"Title\"] or \"\") + \" \" + (issue_dict[random_id][\"Description\"] or \"\")\n                    pairs.append(InputExample(\n                        texts = [bug_1, bug_2],\n                        label = 0\n                    ))\n    \n    return pairs\n\nfrom concurrent.futures import ThreadPoolExecutor\n\ndatasets = [\n    (\"eclipse_train\", eclipse_train[0:100], eclipse_dict),\n    (\"eclipse_test\", eclipse_test[0:20], eclipse_dict),\n    (\"firefox_train\", firefox_train[0:100], firefox_dict),\n    (\"firefox_test\", firefox_test[0:20], firefox_dict),\n    (\"jdt_train\", jdt_train[0:100], jdt_dict),\n    (\"jdt_test\", jdt_test[0:20], jdt_dict),\n    (\"mozilla_train\", mozilla_train[0:100], mozilla_dict),\n    (\"mozilla_test\", mozilla_test[0:20], mozilla_dict),\n    (\"thunderbird_train\", thunderbird_train[0:100], thunderbird_dict),\n    (\"thunderbird_test\", thunderbird_test[0:20], thunderbird_dict),\n]\n\nresults = {}\n\ndef process_dataset(name, data, dictionary):\n    return name, create_pairs(data, dictionary)\n\nwith ThreadPoolExecutor() as executor:\n    futures = {executor.submit(process_dataset, name, data, dictionary): name for name, data, dictionary in datasets}\n    \n    for future in futures:\n        name, result = future.result()\n        results[name] = result\n\n# Assign results to variables dynamically\neclipse_train_samples = results[\"eclipse_train\"]\neclipse_test_samples = results[\"eclipse_test\"]\nfirefox_train_samples = results[\"firefox_train\"]\nfirefox_test_samples = results[\"firefox_test\"]\njdt_train_samples = results[\"jdt_train\"]\njdt_test_samples = results[\"jdt_test\"]\nmozilla_train_samples = results[\"mozilla_train\"]\nmozilla_test_samples = results[\"mozilla_test\"]\nthunderbird_train_samples = results[\"thunderbird_train\"]\nthunderbird_test_samples = results[\"thunderbird_test\"]\n\ntrain_samples = eclipse_train_samples + firefox_train_samples + jdt_train_samples + mozilla_train_samples + thunderbird_train_samples\ntest_samples =  eclipse_test_samples + firefox_test_samples + jdt_test_samples + mozilla_test_samples + thunderbird_test_samples"},{"cell_type":"code","execution_count":47,"id":"61afb009-4b7c-4193-95da-f823f58ed968","metadata":{"execution":{"iopub.execute_input":"2025-03-26T03:56:20.217694Z","iopub.status.busy":"2025-03-26T03:56:20.217396Z","iopub.status.idle":"2025-03-26T04:36:00.437537Z","shell.execute_reply":"2025-03-26T04:36:00.437008Z","shell.execute_reply.started":"2025-03-26T03:56:20.217670Z"},"language":"python","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":"Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"},"metadata":{},"output_type":"display_data"},{"data":{"text/html":"\n    <div>\n      \n      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [154/154 39:30, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>","text/plain":"<IPython.core.display.HTML object>"},"metadata":{},"output_type":"display_data"}],"source":"# NORMAL FINE-TUNING\n# Adjust Learning Rate, Weight decay\n\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import losses\n\ntrain_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16, worker_init_fn=lambda _: set_seed(SEED))\nmodel = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n#if torch.cuda.device_count()>1:\n#    print (f\"Using{torch.cuda.device_count()} GPUs!\")\n#    model = torch.nn.DataParallel(model)\n#model.to(device)\ntrain_loss = losses.CosineSimilarityLoss(model)\nnum_epochs = 2\nwarmup_steps = int(0.05 * len(train_dataloader) * num_epochs)\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=num_epochs,\n    warmup_steps=warmup_steps,\n    use_amp=True,\n    show_progress_bar=True,\n    optimizer_params={\"lr\": 2e-5, \"weight_decay\": 0.01}  # Adjust these values\n)"},{"cell_type":"code","execution_count":34,"id":"b7dab836-1a7c-4489-ad5d-b21d70448aa4","metadata":{"execution":{"iopub.execute_input":"2025-03-26T03:27:38.623059Z","iopub.status.busy":"2025-03-26T03:27:38.622325Z","iopub.status.idle":"2025-03-26T03:51:10.553492Z","shell.execute_reply":"2025-03-26T03:51:10.553052Z","shell.execute_reply.started":"2025-03-26T03:27:38.623025Z"},"language":"python","scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":"Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"},"metadata":{},"output_type":"display_data"},{"data":{"text/html":"\n    <div>\n      \n      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [154/154 23:22, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>","text/plain":"<IPython.core.display.HTML object>"},"metadata":{},"output_type":"display_data"}],"source":"#ADAPTER TRAINING\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import SentenceTransformer, losses\nimport adapters\n\ntrain_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16, worker_init_fn=lambda _: set_seed(SEED))\nmodel = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Add an adapter for fine-tuning\nadapter_name = \"pfeiffer_adapter\"\n\n# Get the underlying Hugging Face transformer model\ntransformer_model = model._first_module().auto_model \n\n# Add and activate adapter\nadapters.init(transformer_model)\ntransformer_model.add_adapter(adapter_name, config=\"pfeiffer\", set_active=True)\ntransformer_model.train_adapter(adapter_name)\n\ntrain_loss = losses.CosineSimilarityLoss(model)\nnum_epochs = 2\nwarmup_steps = int(0.05 * len(train_dataloader) * num_epochs)\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=num_epochs,\n    warmup_steps=warmup_steps,\n    use_amp=True,\n    show_progress_bar=True,\n    optimizer_params={\"lr\": 2e-5, \"weight_decay\": 0.01}  # Learning rate and weight decay\n)\n"},{"cell_type":"code","execution_count":8,"id":"19febf0d-8bf7-4290-af25-2098a791991d","metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:37:47.380242Z","iopub.status.busy":"2025-03-26T05:37:47.379815Z","iopub.status.idle":"2025-03-26T06:25:37.248748Z","shell.execute_reply":"2025-03-26T06:25:37.247999Z","shell.execute_reply.started":"2025-03-26T05:37:47.380202Z"},"language":"python","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b18b95e4d0034471beea7694fe374fa9","version_major":2,"version_minor":0},"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1e80d0dbab14a6db04529dcc1b9a879","version_major":2,"version_minor":0},"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd993addf0da4aa0a9d9ba1adcc4871e","version_major":2,"version_minor":0},"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8087517ea1d3492e8a23c03c9f0d4655","version_major":2,"version_minor":0},"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a20648d2d67b475cb40527e7762a6346","version_major":2,"version_minor":0},"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"440e28f715ea4d96a619f6694f78fec7","version_major":2,"version_minor":0},"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"452ad1116aed4f779449acc5b9d4b7d2","version_major":2,"version_minor":0},"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b26eea0717943c7a5b4de30bc339ee7","version_major":2,"version_minor":0},"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38b70a1c76004f1d9b135e55951c6a38","version_major":2,"version_minor":0},"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cef73796efc4230b864d6e411e34726","version_major":2,"version_minor":0},"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a964b73bf7c842e08f07e38b526b9520","version_major":2,"version_minor":0},"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":"Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":"Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"},"metadata":{},"output_type":"display_data"},{"data":{"text/html":"\n    <div>\n      \n      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [154/154 47:31, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>","text/plain":"<IPython.core.display.HTML object>"},"metadata":{},"output_type":"display_data"}],"source":"#LAYER SPECIFIC FINE TUNING\n\nimport torch\nfrom sentence_transformers import SentenceTransformer, losses\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16, worker_init_fn=lambda _: set_seed(SEED))\nmodel = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Optionally freeze lower layers (e.g., layers 0 to 5)\nfor name, param in model.named_parameters():\n    if '0' <= name.split('.')[1] <= '5':  # Assuming the layers you want to freeze are in range 0 to 5\n        param.requires_grad = False\n\ntrain_loss = losses.CosineSimilarityLoss(model)\nnum_epochs = 2\nwarmup_steps = int(0.05 * len(train_dataloader) * num_epochs)\n\n# Train the model with layer-specific fine-tuning\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=num_epochs,\n    warmup_steps=warmup_steps,\n    use_amp=True,\n    show_progress_bar=True,\n    optimizer_params={\"lr\": 2e-5, \"weight_decay\": 0.01}  # Adjust these values\n)\n"},{"cell_type":"code","execution_count":28,"id":"c4d814e0-293f-47f8-bd5d-53d0bf5128bc","metadata":{"execution":{"iopub.execute_input":"2025-03-26T06:27:15.202404Z","iopub.status.busy":"2025-03-26T06:27:15.201786Z","iopub.status.idle":"2025-03-26T06:31:45.423646Z","shell.execute_reply":"2025-03-26T06:31:45.421658Z","shell.execute_reply.started":"2025-03-26T06:27:15.202348Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Precision: 0.9524\nRecall: 0.7018\nF1-score: 0.8081\nF2-score: 0.7407\n"}],"source":"from sentence_transformers import util\nimport torch\n\n# Function to evaluate test samples\ndef evaluate_model(model, test_samples):\n    TP, FP, FN = 0, 0, 0\n    total = len(test_samples)\n\n    for sample in test_samples:\n        text1, text2 = sample.texts\n        label = sample.label  # Ground truth (1 for duplicate, 0 for non-duplicate)\n\n        # Compute embeddings\n        emb1 = model.encode(text1, convert_to_tensor=True)\n        emb2 = model.encode(text2, convert_to_tensor=True)\n\n        # Compute cosine similarity\n        similarity = util.pytorch_cos_sim(emb1, emb2).item()\n\n        # Threshold (adjust based on your dataset)\n        prediction = 1 if similarity > 0.5 else 0\n\n        # Compare with ground truth\n        # Count TP, FP, FN\n        if prediction == 1 and label == 1:\n            TP += 1  # True Positive\n        elif prediction == 1 and label == 0:\n            FP += 1  # False Positive\n        elif prediction == 0 and label == 1:\n            FN += 1  # False Negative\n\n\n    # Compute metrics\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    f2_score = (5 * precision * recall) / ((4 * precision) + recall) if (precision + recall) > 0 else 0 \n    \n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-score: {f1_score:.4f}\")\n    print(f\"F2-score: {f2_score:.4f}\") \n\n# Run evaluation\nevaluate_model(model, test_samples)"},{"cell_type":"code","execution_count":null,"id":"97c29563-417d-49ae-9094-ced35b653372","metadata":{"language":"python","trusted":true},"outputs":[],"source":""}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"","defaultDatabase":""},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}